---
layout: distill
title: On Singular Value Decomposition
description: a small self-note on SVD
tags: distill formatting
giscus_comments: false
date: 2025-05-03
featured: true
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

authors:
  - name: Parag Sarvoday Sahu
  #   url: "https://en.wikipedia.org/wiki/Albert_Einstein"
  #   affiliations:
  #     name: IAS, Princeton
  # - name: Boris Podolsky
  #   url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
  #   affiliations:
  #     name: IAS, Princeton
  # - name: Nathan Rosen
  #   url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
  #   affiliations:
  #     name: IAS, Princeton

bibliography: 2025-05-03-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Definition
  - name: Geometric Interpretation
  - name: Image Compression

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Definition
It is way of decomposition any given matrix $ A \in \mathbb{R}^{m \times n} $ into three components which are given as:

$$
A = U \Sigma V^T
$$

where:
- $ U \in \mathbb{R}^{m \times m} $ is an **orthogonal** matrix ($U^{T}U = I$) 
- $ V \in \mathbb{R}^{n \times n} $ is an **orthogonal** matrix ($V^{T}V = I$)
- $ \Sigma \in \mathbb{R}^{m \times n} $ is a diagonal matrix with **non-negative real numbers** (called **singular values**) on the diagonal

The singular values in $ \Sigma $ are sorted in decreasing order:

$$
\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r > 0
$$

where $ r = \text{rank}(A) $, and the remaining diagonal entries are 0

Remember: Maximum rank of $ A \in \mathbb{R}^{m \times n} $ can be $ \text{min}(m,n)$

$$
\underbrace{
\begin{bmatrix}
\phantom{0} & \phantom{0} & \phantom{0} & \phantom{0} \\
\phantom{0} & \phantom{0} & \phantom{0} &  \phantom{0} \\
\phantom{0} & \phantom{0} & \phantom{0} & \phantom{0} \\
\phantom{0} & \phantom{0} & \phantom{0} & \phantom{0} \\
\phantom{0} & \phantom{0} & \phantom{0} & \phantom{0} \\
\phantom{0} & \phantom{0} & \phantom{0} & \phantom{0}
\end{bmatrix}
}_{A\ (m \times n)}
=
\underbrace{
\begin{bmatrix}
\phantom{0} & \phantom{0} & \phantom{0} & \phantom{0} \\
\phantom{0} & \phantom{0} & \phantom{0} & \phantom{0} \\
\phantom{0} & \phantom{0} & \phantom{0} & \phantom{0} \\
\phantom{0} & \phantom{0} & \phantom{0} & \phantom{0}
\end{bmatrix}
}_{U\ (m \times m)}
\underbrace{
\begin{bmatrix}
\sigma_1 & 0 & 0        \\
0  & \ddots & 0 \\
0        & 0 & \sigma_{r} \\
0        & 0 & 0       \\
0        & 0 & 0
\end{bmatrix}
}_{\Sigma\ (m \times n)}
\underbrace{
\begin{bmatrix}
\phantom{0} & \phantom{0} \\
\phantom{0} & \phantom{0}
\end{bmatrix}
}_{V^{T}\ (n \times n)}
$$



## Geometric Interpretation 

We use the following geometric fact that:
<div style="text-align:center; font-style:italic;">
The image of the unit sphere under any $m \times n$ matrix is a hyperellipse.
</div>

<br>

Let $S$ be the unit sphere in $\mathbb{R}^n$, and take any $A \in \mathbb{R}^{m \times n}$ with $m \geq n$. For simplicity, we take that $A$ has full rank $n$. The image $AS$ is a hyperellipse in $\mathbb{R}^m$. Now, the $n$ singular values of $A$ are the lengths of the $n$ principal semiaxes of $AS$, written $\sigma_1, \sigma_2, \ldots, \sigma_n$.
The $n$ 'left singular' vectors of $A$ unit vectors $\{u_1, u_2, \ldots, u_n\}$ are oriented in the directions of the principal semiaxes of $AS$. The $n$ 'right singular' vectors of $A$ are the unit vectors $\{v_1, v_2, \ldots, v_n\} \in S$ so that $Av_j = \sigma_j u_j$. These $v_j$ and $u_j$ are the vectors which are arranged column-wise in the matrices $V$ and $U$.

<div class="row mt-3">
    <div class="col-18 mx-auto mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/SVD_blog/geometric_meaning.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    SVD of a $2 \times 2$ matrix.
</div>


## Image Compression
One application of Singular Value Decomposition is for image compression.
A grayscale image can be represented as a matrix $A \in \mathbb{R}^{m \times n}$, where each element $a_{ij}$ corresponds to the intensity (0--255) of a pixel.


$$
A_{m \times n} = U_{m \times m}\, \Sigma_{m \times n}\, V_{n \times n}^{T}
$$

Let's take an example image which has $500$ singular values for its original grayscale form.

<div class="row mt-3">
    <div class="col-6 mx-auto mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/SVD_blog/dog_original.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Original grayscale image.
</div>

Now, we can approximate $A$ by keeping only the top $k$ singular values and their corresponding singular vectors.\\
This gives a $\text{rank}-k$ approximation:

$$
A_k = U_k \Sigma_k V_k^T
$$

where:
- $U_k \in \mathbb{R}^{m \times k}$ contains the first $k$ columns of $U$
- $\Sigma_k \in \mathbb{R}^{k \times k}$ contains the top $k$ singular values
- $V_k \in \mathbb{R}^{n \times k}$ contains the first $k$ columns of $V$

The original matrix requires storing $(m \times n)$ values but with the SVD approximation:

If $k \ll \min(m,n):$

Total values stored $= mk + k + nk = k(m + n + 1)$ which is $\ll (m \times n)$

For much smaller value of $k$ (in comparison to the original number of singular values which is $500$ in this case), much of the features of the original image are preserved.  

<div class="row mt-3">
    <div class="col-30 mx-auto mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/SVD_blog/dog_reconstructed.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Reconstructed images.
</div>

Here, $\text{RE}$ stands for Reconstruction Error which is found by taking the Mean Squared Error between the original and the reconstructed image.













