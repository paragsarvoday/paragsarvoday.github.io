<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://paragsarvoday.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://paragsarvoday.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-10T17:53:39+00:00</updated><id>https://paragsarvoday.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Understanding 3D Gaussian Splatting</title><link href="https://paragsarvoday.github.io/blog/2025/Understanding-3D-Gaussian-Splatting/" rel="alternate" type="text/html" title="Understanding 3D Gaussian Splatting"/><published>2025-08-30T00:00:00+00:00</published><updated>2025-08-30T00:00:00+00:00</updated><id>https://paragsarvoday.github.io/blog/2025/Understanding-3D-Gaussian-Splatting</id><content type="html" xml:base="https://paragsarvoday.github.io/blog/2025/Understanding-3D-Gaussian-Splatting/"><![CDATA[<h2 id="what-exactly-is-this-technique">What exactly is this technique?</h2> <p>3D Gaussian Splatting is one of the solutions to the <strong>Novel View Synthesis</strong> problem.</p> <p>Novel View Synthesis? That seems quite complicated, I don’t think this is my cup of tea.</p> <p>Well, don’t stop reading just yet. Its not as contrived as it seems at first glance. Let’s start by understanding the literal meaning of the term ‘Novel View Synthesis’. It is made up of <em>novel</em> which means <em>new</em>, view is quite self-explanatory and <em>synthesis</em> means <em>to combine different elements to make something new</em>. So, the process goes something like this, we have a bunch of pictures of a room (say) and now we want to view the room from angles which were not present in the original set of pictures. To fulfill this desire of ours, we need to <em>synthesize</em> (make) these <em>novel</em> (new) views.</p> <p>Now, there had been many attempts to solve the Novel View Synthesis problem but none of the attempts made for a convincing solution until 3D Gaussian Splatting (3DGS) appeared in 2023. Let’s understand how it works!</p> <h2 id="foundation">Foundation</h2> <p>Before moving to the actual workings of 3DGS, let’s lay a firm foundational knowledge about gaussians.</p> <h3 id="1d-gaussian-function">1D Gaussian function</h3> <p>Starting off with 1D gaussian function, its general form is given as:</p> \[g(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{1}{2} \frac{(x - \mu)^2}{\sigma^2} \right)\] <p>Here, $\sigma$ is the spread of the gaussian and $\mu$ is the centre value of it. We query the function by substituting a value in place of the variable $x$.</p> <p>Check out the visualisation below for a better understanding:</p> <div class="row mt-3"> <div class="col-12 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3dgs_blog/1dgs-480.webp 480w,/assets/img/3dgs_blog/1dgs-800.webp 800w,/assets/img/3dgs_blog/1dgs-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/3dgs_blog/1dgs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A 1D gaussian function </div> <details><summary>An interesting detail about 1D gaussians</summary> <p>Gaussian functions arise by composing the exponential function with a concave quadratic function (downward facing parabola)</p> \[f(x) = \exp(\alpha x^2 + \beta x + \gamma)\] <p>where \(\alpha = -\frac{1}{2\sigma^2}, \quad \beta = \frac{\mu}{\sigma^2}, \quad \gamma = -\frac{\mu^2}{2\sigma^2} - \ln(\sigma \sqrt{2\pi})\)</p> </details> <h3 id="2d-gaussian-function">2D Gaussian function</h3> <p>The 2D Gaussian function in our familiar form would look something like:</p> \[g(x, y) = A \exp \left( - \left( \frac{(x - \mu_x)^2}{2\sigma_X^2} + \frac{(y - \mu_y)^2}{2\sigma_Y^2} \right) \right)\] <p>Here, $A$ is the amplitude, $\mu_x$, $\mu_y$ are the centers, and $\sigma_X$, $\sigma_Y$ are the spreads of the 2D gaussian in the $X$ and $Y$ axes respectively.</p> <p>Again, a visualisation to help form a mental picture:</p> <div class="row mt-3"> <div class="col-12 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3dgs_blog/2dgs_final-480.webp 480w,/assets/img/3dgs_blog/2dgs_final-800.webp 800w,/assets/img/3dgs_blog/2dgs_final-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/3dgs_blog/2dgs_final.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A 2D gaussian function </div> <p>Notice that a 2D Gaussian is built by stacking ellipses on top of each other. The set of 2D points which result in the same output when plugged in the 2D Gaussian function lie on the same ellipse.</p> <p>So, when a 2D Gaussian is sliced with a plane parallel to the $XY$ plane, we get an ellipse.</p> <h3 id="3d-gaussian-function">3D Gaussian function</h3> <p>As a natural extension to the 2D Gaussian’s definition, we get to the 3D Gaussian function which is given as:</p> \[g(x, y, z) = A \cdot \exp\left( -\frac{1}{2} \left( \frac{(x - \mu_x)^2}{\sigma_x^2} + \frac{(y - \mu_y)^2}{\sigma_y^2} + \frac{(z - \mu_z)^2}{\sigma_z^2} \right) \right)\] <p>An alternate definition of a 3D Gaussian that is often seen in various place (eg. the 3DGS paper!) goes like:</p> \[f(x, y, z) = A \cdot \exp\left( -\frac{1}{2} (\mathbf{r} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{r} - \boldsymbol{\mu}) \right)\] <p>where:</p> <p>$A = (2\pi)^{-3/2} \det(\Sigma)^{-1/2}$ is the normalization constant</p> <p>$\mathbf{r} = [x, y, z]^\top$ is the position vector</p> <p>$\boldsymbol{\mu} = [\mu_x, \mu_y, \mu_z]^\top$ is the center of the Gaussian</p> <p>$\Sigma^{-1}$ is the inverse of the covariance matrix</p> <details><summary>How can these two definitions be equivalent?</summary> <p>I won’t give out a formal proof to establish their equivalence, but here’s a simple example to build an intuition about it.</p> <p>We take $\Sigma$ to be a diagonal matrix:</p> \[\Sigma = \begin{bmatrix} \sigma_x^2 &amp; 0 &amp; 0 \\ 0 &amp; \sigma_y^2 &amp; 0 \\ 0 &amp; 0 &amp; \sigma_z^2 \end{bmatrix}\] <p>then</p> <p>then \(\Sigma^{-1} = \begin{bmatrix} \frac{1}{\sigma_x^2} &amp; 0 &amp; 0 \\ 0 &amp; \frac{1}{\sigma_y^2} &amp; 0 \\ 0 &amp; 0 &amp; \frac{1}{\sigma_z^2} \end{bmatrix}\)</p> <p>because for a diagonal matrix, the inverse is simply the reciprocal of each of the diagonal elements.</p> <p>expanding $(\mathbf{r} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{r} - \boldsymbol{\mu})$, we substitute $\mathbf{r} = [x, y, z]^\top$ and $\boldsymbol{\mu} = [\mu_x, \mu_y, \mu_z]^\top$</p> <p>we get:</p> \[(\mathbf{r} - \boldsymbol{\mu}) = \begin{bmatrix} x - \mu_x \\ y - \mu_y \\ z - \mu_z \end{bmatrix}\] \[\Sigma^{-1} (\mathbf{r} - \boldsymbol{\mu}) = \begin{bmatrix} \frac{1}{\sigma_1^2} &amp; 0 &amp; 0 \\ 0 &amp; \frac{1}{\sigma_2^2} &amp; 0 \\ 0 &amp; 0 &amp; \frac{1}{\sigma_3^2} \end{bmatrix} \begin{bmatrix} x - \mu_x \\ y - \mu_y \\ z - \mu_z \end{bmatrix} = \begin{bmatrix} \frac{x - \mu_x}{\sigma_x^2} \\ \frac{y - \mu_y}{\sigma_y^2} \\ \frac{z - \mu_z}{\sigma_z} \end{bmatrix}\] <p>now, \((\mathbf{r} - \boldsymbol{\mu})^\top \begin{bmatrix} \frac{(x - \mu_x)}{\sigma_x^2} \\ \frac{(y - \mu_y)}{\sigma_y^2} \\ \frac{(z - \mu_z)}{\sigma_z^2} \end{bmatrix} = \frac{(x - \mu_x)^2}{\sigma_x^2} + \frac{(y - \mu_y)^2}{\sigma_y^2} + \frac{(z - \mu_z)^2}{\sigma_z^2}\)</p> <p>which looks like the term inside the exponential!</p> </details> <details><summary>Equation for N-dimensional Gaussian function</summary> \[f(\mathbf{x}) = \frac{1}{(2\pi)^{N/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^\mathsf{T} \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})\right)\] <p>Here, $\mathbf{x}$ will belong to $\mathbb{R}^{N}$ and $\boldsymbol{\Sigma}$ will be of dimension $N \times N$</p> </details> <p>For 3D Gaussian, the $\Sigma$ matrix would look something like this:</p> \[\Sigma = \begin{bmatrix} \sigma_{xx} &amp; \sigma_{xy} &amp; \sigma_{xz} \\ \sigma_{xy} &amp; \sigma_{yy} &amp; \sigma_{yz} \\ \sigma_{xz} &amp; \sigma_{yz} &amp; \sigma_{zz} \end{bmatrix}\] <p>where:</p> <ul> <li>the diagonal elements $(\sigma_{xx}$, $\sigma_{yy}$, $\sigma_{zz})$ represent the variances along the $x$, $y$ and $z$ axes.</li> <li>the off-diagonal elements $(\sigma_{xy}, \sigma_{xz}, \sigma_{yz})$ represent covariances between pairs of axes $(x, y, z)$.</li> </ul> <p>A 3D Gaussian cannot be visualised the way we did for the other two lower dimensional Gaussians, because we cannot directly visualise a 4D space.</p> <p>As an extension from the ellipse idea for 2D Gaussians, the set of 3D points which result in the same output from the 3D Gaussian function would lie on an <strong>ellipsoid</strong>. It can also be said that a 3D Gaussian is built from many ellipsoids where the larger ellipsoids enclose the smaller ones within them.</p> <p>Nonetheless, here’s an illustration of an ellipsoid just to have a mental picture:</p> <div class="row mt-3"> <div class="col-12 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3dgs_blog/3D_Ellipsoid-480.webp 480w,/assets/img/3dgs_blog/3D_Ellipsoid-800.webp 800w,/assets/img/3dgs_blog/3D_Ellipsoid-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/3dgs_blog/3D_Ellipsoid.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A 'slice' of a 3D Gaussian, an ellipsoid </div> <p>If we perform the eigenvalue decomposition of the $\Sigma$ matrix, we get:</p> \[\Sigma = Q \Lambda Q^\top\] <p>where:</p> <ul> <li>$Q$: A matrix whose columns are the <strong>eigenvectors defining orientation</strong> of the ellipsoid wrt to the $x$, $y$ and $z$ axes.</li> <li>$\Lambda$: A diagonal matrix containing <strong>eigenvalues defining stretching</strong> along the $x$, $y$ and $z$ axes.</li> </ul> <h2 id="key-idea">Key Idea</h2> <p>The general way to get <em>novel</em> view of any scene is to get a 3D representation of it, be it a mesh based, or point cloud based or a voxel based representation. Then using this 3D representation, render the scene from any viewpoint of your choice. This is basically like simulating the world (scene) inside our computers and then viewing it however we want.</p> <p>The problem is that these 3D representations of a scene cannot be reliably estimated from just a bunch of pictures of the scene. This is where the magic of 3D Gaussian Splatting comes in!</p> <p>Here, we represent the scene using 3D Gaussians. These Gaussians when projected and rendered on a 2D plane, give us a picture of the scene from a particular viewpoint. To be able to get a good picture of the scene from these Gaussians, the Gaussians are subjected to an interative optimization process. As part of the optimization process, we render the scene from the viewpoints for which we have the original images, and then take a photometric loss between the renderings and the original images.</p> <p>Based on the difference between the renderings and the original images, we tweak the properties (what are these properties?) of the 3D Gaussians such that their difference is minimised.</p> <div class="row mt-3"> <div class="col-12 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3dgs_blog/3dgs_pipeline-480.webp 480w,/assets/img/3dgs_blog/3dgs_pipeline-800.webp 800w,/assets/img/3dgs_blog/3dgs_pipeline-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/3dgs_blog/3dgs_pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The overall pipeline of the 3DGS framework </div> <h2 id="properties-of-a-3d-gaussian">Properties of a 3D Gaussian</h2> <p>The following are the properties attached with each of the 3D Gaussians in the 3DGS framework:</p> <ol> <li>3D position ($\boldsymbol{\mu}$): A 3D vector which signifies the exact location where the 3D Gaussian is placed in space.</li> <li>Opacity ($\alpha$): Gives how much of the colour of this Gaussian influences the final image. An opacity value of $0$ means that the Gaussian is completely transparent i.e. it does not contribute anything to the final image and an opacity value of $1$ means that the Gaussian is completely opaque i.e. nothing behind it is visible in the final image.</li> <li>Anisotropic covariance ($\Sigma$): This gives the scale (size) and rotation (orientation) of the 3D Gaussian. It is called anisotropic because it is not symmetric in the three directions. An isotropic covariance having 3D Gaussian will just be a sphere with varying size (here, $\Sigma = k\boldsymbol{I}$).</li> <li>SH coefficients: These give the colour of the 3D Gaussian as a function of the viewing direction.</li> </ol> <h2 id="projection-of-3d-gaussians-onto-a-2d-plane">Projection of 3D Gaussians onto a 2D plane</h2> <p>The 3DGS paper directly hands out the following relation between the covariance of the 3D Gaussian ($\Sigma_{3D}$) and the 2D Gaussian ($\Sigma_{2D}$) obtained after projection:</p> \[\begin{gathered} \Sigma_{2D} = JW \Sigma_{3D} W^{T}J^{T} \\ \text{(yes, this cannot be used directly)} \end{gathered}\] <p>We will now try to understand where this stems from instead of taking things just as given. Afterall, this is what researchers do!</p> <p>This part is a little more involved than what we have seen so far but hang on and we will get through it.</p> <p>An important property to know of about Gaussians is:</p> <div style="text-align:center; font-weight:bold;"> Gaussian functions are closed under affine transformations. </div> <p><br/></p> <p>Let’s break down the above property step-by-step.</p> <p>Affine transformations are of the following form:</p> \[f(\vec{x}) = A\vec{x} + \vec{b}\] <p>And when Gaussian functions are subjected to an affine transformation, this again results in a Gaussian function. Hence, they are called closed under affine transformation.</p> <p>Don’t believe me? Here’s the proof for you.</p> <p>We re-write the affine transformation as: \(\vec{x} = A^{-1}(\vec{y} - \vec{b})\)</p> <p>Then we substitute this in the equation for 3D Gaussian:</p> \[\begin{align*} Q(\vec{y}) &amp;= \exp\left(-\frac{1}{2} \left(A^{-1}(\vec{y} - \vec{b}) - \vec{\mu}\right)^T \Sigma^{-1} \left(A^{-1}(\vec{y} - \vec{b}) - \vec{\mu}\right)\right) \\ &amp;= \exp\left(-\frac{1}{2} \left(A^{-1}(\vec{y} - (A\vec{\mu} + \vec{b}))\right)^T \Sigma^{-1} \left(A^{-1}(\vec{y} - (A\vec{\mu} + \vec{b}))\right)\right) \\ &amp;= \exp\left(-\frac{1}{2} \left(\vec{y} - {\color{blue}\underbrace{(A\vec{\mu} + \vec{b})}}\right)^T {\color{purple}\underbrace{A^{-T} \Sigma^{-1} A^{-1}}} \left(\vec{y} - {\color{blue}\underbrace{(A\vec{\mu} + \vec{b})}}\right)\right) \\ &amp;= \exp\left(-\frac{1}{2} (\vec{y} - {\color{blue}\vec{\mu}'})^T {\color{purple}\Sigma'} (\vec{y} - {\color{blue}\vec{\mu}'})\right) \end{align*}\] <p>See, even after applying the affine transformation, we end up with an equation for a Gaussian. Albeit with a different centre and covariance matrix (the transformation had to do something right?).</p> <p>You might remember seeing the following kind of illustration back in your computer vision class. This shows the need for world-to-camera coordinate system transformation.</p> <div class="row mt-3"> <div class="col-12 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3dgs_blog/w2c-480.webp 480w,/assets/img/3dgs_blog/w2c-800.webp 800w,/assets/img/3dgs_blog/w2c-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/3dgs_blog/w2c.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Typical world-to-camera coordinate system transformation. </div> <p>To simplify our case of projecting 3D Gaussians onto a 2D plane, we will consider that there is no translation involved in going from the world coordinate system to the camera coordinate system and just a rotation gets the job done.</p> <p>We define the viewing transformation $W$ as the transformation which gets us from the world space to the camera space.</p> <p>Now, we apply the viewing transformation onto the 3D Gaussian function:</p> \[G(\vec{x}) = \exp\left\{-\frac{1}{2} (W\vec{x} - W\vec{\mu})^T (W\Sigma W^T)^{-1} (W\vec{x} - W\vec{\mu})\right\}\] <details><summary>How can we directly apply the viewing transformation on the covariance matrix? Isnt it meant for a point?</summary> <p>The covariance matrix is given as:</p> \[\Sigma = E[(\vec{x} - \vec{\mu})(\vec{x} - \vec{\mu})^{T}]\] <p>After applying the transformation $W$ on the vector $\vec{x}$:</p> \[\begin{align} \vec{x'} &amp;= W\vec{x} \\ \vec{\mu'} &amp;= W\vec{\mu} \end{align}\] <p>Now,</p> \[% Preamble: \usepackage{amsmath}, \usepackage{xcolor} \begin{align*} \Sigma' &amp;= E\left[(\vec{x}' - \vec{\mu}')(\vec{x}' - \vec{\mu}')^T\right] \\ &amp;= E\left[(W(\vec{x} - \vec{\mu}))(W(\vec{x} - \vec{\mu}))^T\right] \\ &amp;= E\left[W(\vec{x} - \vec{\mu})(\vec{x} - \vec{\mu})^T W^T\right] \end{align*}\] <p>$W$ is a constant transformation matrix. Hence, can be directly taken out from the expectation expression.</p> \[\begin{align*} \Sigma' &amp;= W {\color{blue}{\underbrace{E\left[(\vec{x} - \vec{\mu})(\vec{x} - \vec{\mu})^T \right]}}} W^T \\ &amp;= W {\color{blue}\Sigma} W^{T} \\ \end{align*}\] <p>This justifies our application of the viewing transformation directly on the covariance matrix.</p> </details> <p>We now define the following terms in the <em>camera space</em> which will be come in handy in a while:</p> \[\begin{align*} \vec{x'} &amp;= W\vec{x} \\ \vec{\mu'} &amp;= W\vec{\mu} \\ \Sigma' &amp;= W \Sigma W^{T} \end{align*}\] <p>The Gaussians are to be projected in the camera space onto the $z=1$ plane, and the projection function $\varphi$ is defined as (<strong>cite the paper</strong>):</p> \[\varphi(\vec{x'}) = \vec{x'} (\vec{x_0}^{T} \vec{x'})^{-1} (\vec{x_0}^{T} \vec{x_0}) = \vec{x'} (\vec{x_0}^{T} \vec{x'})^{-1}\] <p>where $\vec{x_0} = [0, 0, 1]^{T}$ represents the projection of the camera space’s origin onto the $z=1$ plane.</p> <p>I myself don’t have an intuitive understanding of the above projection function but for the $z=1$ plane case, this gets simplified in a pretty understandable form:</p> \[% Preamble: \usepackage{amsmath}, \usepackage{xcolor} \varphi(\vec{x}') = \vec{x'} \underbrace{ (\vec{x_0}^T \vec{x'})^{-1} }_{\substack{\downarrow \\ \frac{1}{Z}}} \underbrace{ (\vec{x_0}^T \vec{x_0}) }_{\substack{\downarrow \\ 1}}\] <p>And this is simply equivalent to:</p> \[\begin{bmatrix} x \\ y \\ z \end{bmatrix} \rightarrow \begin{bmatrix} x/z \\ y/z \\ 1 \end{bmatrix}\] <p>Another important property to take note of:</p> <div style="text-align:center; font-weight:bold;"> Gaussian functions are closed under affine transformations <span style="color: red;">but not under projective transformation</span>. </div> <p><br/></p> <p>We want to get to get a 2D Gaussian after projection of 3D Gaussian onto the image plane. Hence, we take an affine approximation of the projection function using the Taylor’s series expansion:</p> \[% Preamble: \usepackage{amsmath} \begin{align*} \varphi(\vec{x'}) &amp;= \varphi(\vec{\mu'}) + \frac{\partial\varphi}{\partial\vec{x'}}(\vec{\mu'})(\vec{x'} - \vec{\mu'}) + R_1(\vec{x'}) \\ &amp;\approx \varphi(\vec{\mu'}) + \frac{\partial\varphi}{\partial\vec{x'}}(\vec{\mu'})(\vec{x'} - \vec{\mu'}) \end{align*}\] <p>This gives us:</p> \[% Preamble: % \usepackage{amsmath} % \usepackage{xcolor} % \definecolor{myPink}{RGB}{199, 61, 128} {\color{myPink} \varphi(\vec{x}') = \varphi(\vec{\mu}) + {J}(\vec{x}' - \vec{\mu}') }\] <p>where ${J} = \frac{\partial\varphi}{\partial\vec{x’}}(\vec{\mu’})$ is the Jacobian of the affine approximation of the projective transformation.</p> <p>Applying this approximated projection function to the 3D Gaussian’s function:</p> \[% Preamble: \usepackage{amsmath}, \usepackage{xcolor} \begin{align*} G_{2D}(\vec{x'}) &amp;= \exp\left\{-\frac{1}{2} ({J}\vec{x'} - {J}\vec{\mu'})^T ({J} \Sigma' {J}^T)^{-1} ({J}\vec{x'} - {J}\vec{\mu'})\right\} \\ &amp;\approx \exp\left\{-\frac{1}{2} (\varphi(\vec{x'}) - \varphi(\vec{\mu'}))^T ({\color{purple}\underbrace{J \Sigma' {J}^T}_{\substack{\downarrow \\ \Sigma_{2D}}}})^{-1} (\varphi(\vec{x'}) - \varphi(\vec{\mu'}))\right\} \end{align*}\] <p>Comparing the above equation with that of a standard Gaussian equation, we get the relation for the covariance of the 2D Gaussian ($\Sigma_{2D}$).</p> \[\Sigma_{2D} = J \Sigma' {J}^T = J W \Sigma W^{T} J^{T}\] <p>Now, $\Sigma_{2D}$ must be a $2 \times 2$ matrix as it the covariance matrix of a <em>2D</em> Gaussian. However, the right hand side of the above equation gives out a $3 \times 3$ matrix.</p> <p>What’s at play here?</p> <p>If you have read the paper, you might know that we need to discard the last row and column of the obtained $3 \times 3$ matrix to get our desired $2 \times 2$ covariance matrix.</p> <p>Again, we cannot accept anything just because we were told so. Let’s try getting a better intuition for what’s going on.</p> <p>We have the following defined:</p> \[\begin{align*} \text{A point } \vec{x'} &amp;= [x, y, z]^T \\ \text{Gaussian's centre } \vec{\mu'} &amp;= [\mu_x, \mu_y, \mu_z]^T \\ \text{The projection formula } \varphi(\vec{x'}) &amp;= \begin{bmatrix} x/z \\ y/z \\ 1 \end{bmatrix} \end{align*}\] <p>The expanded Jacobian matrix would look like the following:</p> \[J = \frac{\partial \varphi}{\partial \vec{x'}}(\vec{\mu'}) = \begin{bmatrix} \frac{\partial(x/z)}{\partial x} &amp; \frac{\partial(x/z)}{\partial y} &amp; \frac{\partial(x/z)}{\partial z} \\ \frac{\partial(y/z)}{\partial x} &amp; \frac{\partial(y/z)}{\partial y} &amp; \frac{\partial(y/z)}{\partial z} \\ \frac{\partial(1)}{\partial x} &amp; \frac{\partial(1)}{\partial y} &amp; \frac{\partial(1)}{\partial z} \end{bmatrix}_{\text{at } \vec{x'}=\vec{\mu'}} = \begin{bmatrix} 1/\mu_z &amp; 0 &amp; -\mu_x/\mu_z^2 \\ 0 &amp; 1/\mu_z &amp; -\mu_y/\mu_z^2 \\ 0 &amp; 0 &amp; 0 \end{bmatrix}\] <p>Notice that the rank of the $J$ matrix is $2$ due to the third row being entirely zeros.</p> <p>Coming back,</p> \[\Sigma_{2D} = J \Sigma' J^{T} = \begin{bmatrix} 1/\mu_z &amp; 0 &amp; -\mu_x/\mu_z^2 \\ 0 &amp; 1/\mu_z &amp; -\mu_y/\mu_z^2 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} \begin{bmatrix} \sigma'_{11} &amp; \sigma'_{12} &amp; \sigma'_{13} \\ \sigma'_{21} &amp; \sigma'_{22} &amp; \sigma'_{23} \\ \sigma'_{31} &amp; \sigma'_{32} &amp; \sigma'_{33} \end{bmatrix} \begin{bmatrix} 1/\mu_z &amp; 0 &amp; 0 \\ 0 &amp; 1/\mu_z &amp; 0 \\ -\mu_x/\mu_z^2 &amp; -\mu_y/\mu_z^2 &amp; 0 \end{bmatrix}\] \[\implies \Sigma_{2D} = J\Sigma'J^T = \begin{bmatrix} \sigma_{11} &amp; \sigma_{12} &amp; 0 \\ \sigma_{21} &amp; \sigma_{22} &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix}\] <p>$\Sigma_{2D}$ is the covariance matrix of the new shape in 3D space. But the zeros in the third row and column tell us that there is <em>zero covariance</em> in the 3rd dimension (the z-axis). This makes perfect sense, we flattened the Gaussian onto a 2D plane, so it has no ‘thickness’ or variation along the z-axis anymore. Hence, we can <em>skip the last row and column</em> to get our final $2 \times 2$ covariance matrix $\Sigma_{2D}$ for the 2D Gaussian.</p> <h2 id="way-forward">Way forward</h2> <p>We still haven’t covered some topics such as $\alpha$ blending and Spherical Harmonics. I myself am learning about these and will update the page when I have a good understanding of those. See you soon!</p>]]></content><author><name>Parag Sarvoday Sahu</name></author><category term="math"/><summary type="html"><![CDATA[an attempt to cover the knowledge gap]]></summary></entry><entry><title type="html">A cool image encryption technique</title><link href="https://paragsarvoday.github.io/blog/2025/image-encryption-technique/" rel="alternate" type="text/html" title="A cool image encryption technique"/><published>2025-06-24T00:00:00+00:00</published><updated>2025-06-24T00:00:00+00:00</updated><id>https://paragsarvoday.github.io/blog/2025/image-encryption-technique</id><content type="html" xml:base="https://paragsarvoday.github.io/blog/2025/image-encryption-technique/"><![CDATA[<h2 id="foundation">Foundation</h2> <p>A standard PNG file format image is stored as a 3-dimensional array containing three slices of shape, $\text{height} \times \text{width}$. Each of these slices represent the three colour channels, $\text{Red, Green and Blue}$ which combine to form a familiar images that we see all around us. Now, each of these $\text{height} \times \text{width}$ slices usually contain an $8-bit$ number representing the intensity of the respective colour channel at that pixel’s location.</p> <div class="row mt-3"> <div class="col-8 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image_encryption_blog/rgb-480.webp 480w,/assets/img/image_encryption_blog/rgb-800.webp 800w,/assets/img/image_encryption_blog/rgb-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/image_encryption_blog/rgb.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Illustration of how a standard RGB image is stored. </div> <h2 id="key-idea">Key Idea</h2> <p>The property that we intend to exploit here:</p> <div style="text-align:center; font-style:italic;"> The perceptual quality of an image remains preserved even when represented only with some of its most significant bits (MSBs). </div> <p><br/></p> <p>The above property is better explained with the following example:</p> \[\text{Original 8-bit representation of the image} = B_{7}B_{6}B_{5}B_{4}B_{3}B_{2}B_{1}B_{0}\] \[\text{Image with only} \, n \, \text{of most significant bits} = B_7 B_6 \cdots B_{8 - n} \, \underbrace{0 0 \cdots 0}_{8 - n \text{ bits}}\] <div class="row mt-3"> <div class="col-18 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image_encryption_blog/bit_quantization_comparison-480.webp 480w,/assets/img/image_encryption_blog/bit_quantization_comparison-800.webp 800w,/assets/img/image_encryption_blog/bit_quantization_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/image_encryption_blog/bit_quantization_comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Illustration of how keeping some of the first MSBs affects the image's perceptual quality </div> <p>It is evident that the image retains most of its quality even when just first $4$ of its MSBs are used to represent it.</p> <h2 id="encryption">Encryption</h2> <p>Now, we will exploit the above property for our simple encryption.</p> \[\text{Bit representation in the original visible image} = B_{7}B_{6}B_{5}B_{4}B_{3}B_{2}B_{1}B_{0}\] \[\text{Bit representation in the original image to be encrypted} = b_{7}b_{6}b_{5}b_{4}b_{3}b_{2}b_{1}b_{0}\] \[\text{Bit representation for our encrypted image} = B_{7}B_{6}B_{5}B_{4}b_{7}b_{6}b_{5}b_{4}\] <p>Enough of ideas, let’s see it in action!</p> <div class="row mt-3"> <div class="col-6 mx-auto mt-3 mt-md-0"> <div style="transform: rotate(90deg); display: inline-block; margin-top: 2rem; margin-bottom: 2rem;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image_encryption_blog/flight-480.webp 480w,/assets/img/image_encryption_blog/flight-800.webp 800w,/assets/img/image_encryption_blog/flight-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/image_encryption_blog/flight.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> Original visible image. </div> <div class="row mt-3"> <div class="col-6 mx-auto mt-3 mt-md-0"> <div style="transform: rotate(90deg); display: inline-block; margin-bottom: 2rem;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image_encryption_blog/edison-480.webp 480w,/assets/img/image_encryption_blog/edison-800.webp 800w,/assets/img/image_encryption_blog/edison-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/image_encryption_blog/edison.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> <div class="caption"> Original image to be encrypted. </div> <div class="row mt-3"> <div class="col-6 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/image_encryption_blog/EncryptedImage-480.webp 480w,/assets/img/image_encryption_blog/EncryptedImage-800.webp 800w,/assets/img/image_encryption_blog/EncryptedImage-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/image_encryption_blog/EncryptedImage.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The encrypted image. </div> <p>For decryption, we can simply separate the $4$ most significant and least significant bits from the encrypted image to get a the close approximations of the two original images.</p> <p>Now if we look closely, some parts of the Edison’s image are still visible in the encrypted version. Hence, this method is definitely not full-proof. I suppose we will on the hunt for a good enough encryption technique in the coming few days, together.</p>]]></content><author><name>Parag Sarvoday Sahu</name></author><category term="ImageProcessing"/><summary type="html"><![CDATA[putting bit-planes to good use]]></summary></entry><entry><title type="html">On Singular Value Decomposition</title><link href="https://paragsarvoday.github.io/blog/2025/singular-value-decomposition/" rel="alternate" type="text/html" title="On Singular Value Decomposition"/><published>2025-05-03T00:00:00+00:00</published><updated>2025-05-03T00:00:00+00:00</updated><id>https://paragsarvoday.github.io/blog/2025/singular-value-decomposition</id><content type="html" xml:base="https://paragsarvoday.github.io/blog/2025/singular-value-decomposition/"><![CDATA[<h2 id="definition">Definition</h2> <p>It is way of decomposition any given matrix $ A \in \mathbb{R}^{m \times n} $ into three components which are given as:</p> \[A = U \Sigma V^T\] <p>where:</p> <ul> <li>$ U \in \mathbb{R}^{m \times m} $ is an <strong>orthogonal</strong> matrix ($U^{T}U = I$)</li> <li>$ V \in \mathbb{R}^{n \times n} $ is an <strong>orthogonal</strong> matrix ($V^{T}V = I$)</li> <li>$ \Sigma \in \mathbb{R}^{m \times n} $ is a diagonal matrix with <strong>non-negative real numbers</strong> (called <strong>singular values</strong>) on the diagonal</li> </ul> <p>The singular values in $ \Sigma $ are sorted in decreasing order:</p> \[\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r &gt; 0\] <p>where $ r = \text{rank}(A) $, and the remaining diagonal entries are 0</p> <p>Remember: Maximum rank of $ A \in \mathbb{R}^{m \times n} $ can be $ \text{min}(m,n)$</p> \[\underbrace{ \begin{bmatrix} \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \end{bmatrix} }_{A\ (m \times n)} = \underbrace{ \begin{bmatrix} \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \end{bmatrix} }_{U\ (m \times m)} \underbrace{ \begin{bmatrix} \sigma_1 &amp; 0 &amp; 0 \\ 0 &amp; \ddots &amp; 0 \\ 0 &amp; 0 &amp; \sigma_{r} \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} }_{\Sigma\ (m \times n)} \underbrace{ \begin{bmatrix} \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} \end{bmatrix} }_{V^{T}\ (n \times n)}\] <h2 id="geometric-interpretation">Geometric Interpretation</h2> <p>We use the following geometric fact that:</p> <div style="text-align:center; font-style:italic;"> The image of the unit sphere under any $m \times n$ matrix is a hyperellipse. </div> <p><br/></p> <p>Let $S$ be the unit sphere in $\mathbb{R}^n$, and take any $A \in \mathbb{R}^{m \times n}$ with $m \geq n$. For simplicity, we take that $A$ has full rank $n$. The image $AS$ is a hyperellipse in $\mathbb{R}^m$. Now, the $n$ singular values of $A$ are the lengths of the $n$ principal semiaxes of $AS$, written $\sigma_1, \sigma_2, \ldots, \sigma_n$. The $n$ ‘left singular’ vectors of $A$ unit vectors ${u_1, u_2, \ldots, u_n}$ are oriented in the directions of the principal semiaxes of $AS$. The $n$ ‘right singular’ vectors of $A$ are the unit vectors ${v_1, v_2, \ldots, v_n} \in S$ so that $Av_j = \sigma_j u_j$. These $v_j$ and $u_j$ are the vectors which are arranged column-wise in the matrices $V$ and $U$.</p> <div class="row mt-3"> <div class="col-18 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SVD_blog/geometric_meaning-480.webp 480w,/assets/img/SVD_blog/geometric_meaning-800.webp 800w,/assets/img/SVD_blog/geometric_meaning-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SVD_blog/geometric_meaning.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> SVD of a $2 \times 2$ matrix. </div> <h2 id="image-compression">Image Compression</h2> <p>One application of Singular Value Decomposition is for image compression. A grayscale image can be represented as a matrix $A \in \mathbb{R}^{m \times n}$, where each element $a_{ij}$ corresponds to the intensity (0–255) of a pixel.</p> \[A_{m \times n} = U_{m \times m}\, \Sigma_{m \times n}\, V_{n \times n}^{T}\] <p>Let’s take an example image which has $500$ singular values for its original grayscale form.</p> <div class="row mt-3"> <div class="col-6 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SVD_blog/dog_original-480.webp 480w,/assets/img/SVD_blog/dog_original-800.webp 800w,/assets/img/SVD_blog/dog_original-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SVD_blog/dog_original.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Original grayscale image. </div> <p>Now, we can approximate $A$ by keeping only the top $k$ singular values and their corresponding singular vectors.<br/> This gives a $\text{rank}-k$ approximation:</p> \[A_k = U_k \Sigma_k V_k^T\] <p>where:</p> <ul> <li>$U_k \in \mathbb{R}^{m \times k}$ contains the first $k$ columns of $U$</li> <li>$\Sigma_k \in \mathbb{R}^{k \times k}$ contains the top $k$ singular values</li> <li>$V_k \in \mathbb{R}^{n \times k}$ contains the first $k$ columns of $V$</li> </ul> <p>The original matrix requires storing $(m \times n)$ values but with the SVD approximation:</p> <p>If $k \ll \min(m,n):$</p> <p>Total values stored $= mk + k + nk = k(m + n + 1)$ which is $\ll (m \times n)$</p> <p>For much smaller value of $k$ (in comparison to the original number of singular values which is $500$ in this case), much of the features of the original image are preserved.</p> <div class="row mt-3"> <div class="col-30 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SVD_blog/dog_reconstructed-480.webp 480w,/assets/img/SVD_blog/dog_reconstructed-800.webp 800w,/assets/img/SVD_blog/dog_reconstructed-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SVD_blog/dog_reconstructed.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Reconstructed images. </div> <p>Here, $\text{RE}$ stands for Reconstruction Error which is found by taking the Mean Squared Error between the original and the reconstructed image.</p>]]></content><author><name>Parag Sarvoday Sahu</name></author><category term="math"/><summary type="html"><![CDATA[a small self-note on SVD]]></summary></entry></feed>