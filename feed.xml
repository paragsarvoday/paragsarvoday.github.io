<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://paragsarvoday.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://paragsarvoday.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-04T12:59:03+00:00</updated><id>https://paragsarvoday.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">On Singular Value Decomposition</title><link href="https://paragsarvoday.github.io/blog/2025/distill/" rel="alternate" type="text/html" title="On Singular Value Decomposition"/><published>2025-05-03T00:00:00+00:00</published><updated>2025-05-03T00:00:00+00:00</updated><id>https://paragsarvoday.github.io/blog/2025/distill</id><content type="html" xml:base="https://paragsarvoday.github.io/blog/2025/distill/"><![CDATA[<h2 id="definition">Definition</h2> <p>It is way of decomposition any given matrix $ A \in \mathbb{R}^{m \times n} $ into three components which are given as:</p> \[A = U \Sigma V^T\] <p>where:</p> <ul> <li>$ U \in \mathbb{R}^{m \times m} $ is an <strong>orthogonal</strong> matrix ($U^{T}U = I$)</li> <li>$ V \in \mathbb{R}^{n \times n} $ is an <strong>orthogonal</strong> matrix ($V^{T}V = I$)</li> <li>$ \Sigma \in \mathbb{R}^{m \times n} $ is a diagonal matrix with <strong>non-negative real numbers</strong> (called <strong>singular values</strong>) on the diagonal</li> </ul> <p>The singular values in $ \Sigma $ are sorted in decreasing order:</p> \[\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r &gt; 0\] <p>where $ r = \text{rank}(A) $, and the remaining diagonal entries are 0</p> <p>Remember: Maximum rank of $ A \in \mathbb{R}^{m \times n} $ can be $ \text{min}(m,n)$</p> \[\underbrace{ \begin{bmatrix} \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \end{bmatrix} }_{A\ (m \times n)} = \underbrace{ \begin{bmatrix} \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} &amp; \phantom{0} &amp; \phantom{0} \end{bmatrix} }_{U\ (m \times m)} \underbrace{ \begin{bmatrix} \sigma_1 &amp; 0 &amp; 0 \\ 0 &amp; \ddots &amp; 0 \\ 0 &amp; 0 &amp; \sigma_{r} \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} }_{\Sigma\ (m \times n)} \underbrace{ \begin{bmatrix} \phantom{0} &amp; \phantom{0} \\ \phantom{0} &amp; \phantom{0} \end{bmatrix} }_{V^{T}\ (n \times n)}\] <h2 id="geometric-interpretation">Geometric Interpretation</h2> <p>We use the following geometric fact that:</p> <div style="text-align:center; font-style:italic;"> The image of the unit sphere under any $m \times n$ matrix is a hyperellipse. </div> <p><br/></p> <p>Let $S$ be the unit sphere in $\mathbb{R}^n$, and take any $A \in \mathbb{R}^{m \times n}$ with $m \geq n$. For simplicity, we take that $A$ has full rank $n$. The image $AS$ is a hyperellipse in $\mathbb{R}^m$. Now, the $n$ singular values of $A$ are the lengths of the $n$ principal semiaxes of $AS$, written $\sigma_1, \sigma_2, \ldots, \sigma_n$. The $n$ ‘left singular’ vectors of $A$ unit vectors ${u_1, u_2, \ldots, u_n}$ are oriented in the directions of the principal semiaxes of $AS$. The $n$ ‘right singular’ vectors of $A$ are the unit vectors ${v_1, v_2, \ldots, v_n} \in S$ so that $Av_j = \sigma_j u_j$. These $v_j$ and $u_j$ are the vectors which are arranged column-wise in the matrices $V$ and $U$.</p> <div class="row mt-3"> <div class="col-18 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SVD_blog/geometric_meaning-480.webp 480w,/assets/img/SVD_blog/geometric_meaning-800.webp 800w,/assets/img/SVD_blog/geometric_meaning-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SVD_blog/geometric_meaning.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> SVD of a $2 \times 2$ matrix. </div> <h2 id="image-compression">Image Compression</h2> <p>One application of Singular Value Decomposition is for image compression. A grayscale image can be represented as a matrix $A \in \mathbb{R}^{m \times n}$, where each element $a_{ij}$ corresponds to the intensity (0–255) of a pixel.</p> \[A_{m \times n} = U_{m \times m}\, \Sigma_{m \times n}\, V_{n \times n}^{T}\] <p>Let’s take an example image which has $500$ singular values for its original grayscale form.</p> <div class="row mt-3"> <div class="col-6 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SVD_blog/dog_original-480.webp 480w,/assets/img/SVD_blog/dog_original-800.webp 800w,/assets/img/SVD_blog/dog_original-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SVD_blog/dog_original.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Original grayscale image. </div> <p>Now, we can approximate $A$ by keeping only the top $k$ singular values and their corresponding singular vectors.<br/> This gives a $\text{rank}-k$ approximation:</p> \[A_k = U_k \Sigma_k V_k^T\] <p>where:</p> <ul> <li>$U_k \in \mathbb{R}^{m \times k}$ contains the first $k$ columns of $U$</li> <li>$\Sigma_k \in \mathbb{R}^{k \times k}$ contains the top $k$ singular values</li> <li>$V_k \in \mathbb{R}^{n \times k}$ contains the first $k$ columns of $V$</li> </ul> <p>The original matrix requires storing $(m \times n)$ values but with the SVD approximation:</p> <p>If $k \ll \min(m,n):$</p> <p>Total values stored $= mk + k + nk = k(m + n + 1)$ which is $\ll (m \times n)$</p> <p>For much smaller value of $k$ (in comparison to the original number of singular values which is $500$ in this case), much of the features of the original image are preserved.</p> <div class="row mt-3"> <div class="col-30 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SVD_blog/dog_reconstructed-480.webp 480w,/assets/img/SVD_blog/dog_reconstructed-800.webp 800w,/assets/img/SVD_blog/dog_reconstructed-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SVD_blog/dog_reconstructed.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Reconstructed images. </div> <p>Here, $\text{RE}$ stands for Reconstruction Error which is found by taking the Mean Squared Error between the original and the reconstructed image.</p>]]></content><author><name>Parag Sarvoday Sahu</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[a small self-note on SVD]]></summary></entry></feed>